# Default values for elasticsearch.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.
image:
  repository: "docker.elastic.co/elasticsearch/elasticsearch"
  tag: "5.8.12"
  pullPolicy: "IfNotPresent"

cluster:
  name: "elasticsearch"
  config:
  ## The number of processors is automatically detected, and the thread pool settings are automatically set based on it. 
  ## In some cases it can be useful to override the number of detected processors.
  ## https://www.elastic.co/guide/en/elasticsearch/reference/current/modules-threadpool.html#processors
  #   processors: 2
  env:
  ## Additional master nodes. By default, chart master nodes will be added to zen hosts. But if you want to add other
  ## master nodes to the cluster, you can add it here
  zen:
    hosts:
  #   - "10.148.0.23"

snapshot:
  enabled: false
  name: "elasticsearch"
  schedule: "0 23 * * *"
  restartPolicy: Never

coordinating:
  replicas: 3
  serviceType: ClusterIP
  heapSize: "512m"
  ## antiAffinity
  ## "hard" would be co-locate the pods of service A and service B in the same zone
  ## "sort" would be spread the pods from this service across zones
  antiAffinity: "soft"
  resources:
    limits:
      cpu: "1"
    requests:
      cpu: "100m"
      memory: "728Mi"
  podAnnotations:
    prometheus.io/scrape: "true"
    prometheus.io/port: "9200"
  service:
    ##  service annotations
    ##
    annotations: {}
    type: LoadBalancer
  nodeSelector: {}
  # affinity:
  #   nodeAffinity:
  #     requiredDuringSchedulingIgnoredDuringExecution:
  #       nodeSelectorTerms:
  #       - matchExpressions:
  #         - key: cloud.google.com/gke-nodepool
  #           operator: In
  #           values:
  #           - standard-pool
  ingress:
    enabled: false
    annotations: {}
    #  kubernetes.io/ingress.class: global
    hosts: []
    #  - elasticsearch.domain.com
    tls: []
    #   - secretName: elasticsearch-tls
    #     hosts:
    #       - elasticsearch.domain.com

master:
  replicas: 3
  exposeHttp: false
  heapSize: "256m"
  persistence:
    enabled: true
    accessMode: ReadWriteOnce
    size: "4Gi"
    # storageClass: "ssd"
  ## antiAffinity
  ## "hard" would be co-locate the pods of service A and service B in the same zone
  ## "sort" would be spread the pods from this service across zones
  antiAffinity: "soft"
  resources:
    limits:
      cpu: "1"
    requests:
      cpu: "25m"
      memory: "512Mi"
  podAnnotations:
    prometheus.io/scrape: "true"
    prometheus.io/port: "9200"
  nodeSelector: {}
  # affinity:
  #   nodeAffinity:
  #     requiredDuringSchedulingIgnoredDuringExecution:
  #       nodeSelectorTerms:
  #       - matchExpressions:
  #         - key: cloud.google.com/gke-nodepool
  #           operator: In
  #           values:
  #           - standard-pool

data:
  replicas: 2
  exposeHttp: true
  heapSize: "1024m"
  persistence:
    enabled: true
    accessMode: ReadWriteOnce
    size: "100Gi"
    # storageClass: "ssd"
  terminationGracePeriodSeconds: 3600
  ## antiAffinity
  ## "hard" would be co-locate the pods of service A and service B in the same zone
  ## "sort" would be spread the pods from this service across zones
  antiAffinity: "soft"
  resources:
    limits:
      cpu: "1"
    requests:
      cpu: "300m"
      memory: "1280Mi"
  podAnnotations:
    prometheus.io/scrape: "true"
    prometheus.io/port: "9200"
  nodeSelector: {}
  # affinity:
  #   nodeAffinity:
  #     requiredDuringSchedulingIgnoredDuringExecution:
  #       nodeSelectorTerms:
  #       - matchExpressions:
  #         - key: cloud.google.com/gke-nodepool
  #           operator: In
  #           values:
  #           - high-io-pool


